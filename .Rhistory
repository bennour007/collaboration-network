pacman::p_load(tidyverse, here, gt, rapidraker, httr)
data_raw <- read_csv(here('data', 'scopus18384-2.csv')) %>%
janitor::clean_names()
cols_needed <- c(
'title',
'year',
'authors',
'cited_by',
'author_keywords',
'index_keywords',
'authors_with_affiliations',
'affiliations',
'abstract'
)
data_ided <- data_raw %>%
# colnames()
select(all_of(cols_needed)) %>%
rowid_to_column()
# Your Scopus API Key
api_key <- "de7b2ffad08bb8df27c949ff986c80be"
# DOI for which you want to retrieve data
doi <- "10.1016/j.compstruct.2020.112233" # Replace with your DOI
# Base URL for Scopus API Abstract Retrieval
base_url <- "https://api.elsevier.com/content/article/doi/"
# Prepare the request URL
request_url <- paste0(base_url, doi, "?apiKey=", api_key, "&httpAccept=application/json")
request_url
# Make the GET request
response <- GET(request_url)
response
# Check if the request was successful
if (status_code(response) == 200) {
content_data <- content(response, as = "parsed", type = "application/json")
# Extracting Author Keywords and Index Keywords
author_keywords <- content_data$`abstracts-retrieval-response`$`authkeywords`$keyword
index_keywords <- NA # Scopus API doesn't directly provide "index keywords" in the same way as Web of Science
# Print the keywords
if (!is.null(author_keywords)) {
print("Author Keywords:")
print(sapply(author_keywords, function(k) k$`$`))
} else {
print("No Author Keywords found.")
}
# For index keywords, you might need to look into other parts of the response
# or use specific fields that could be interpreted as index keywords.
} else {
print(paste("Failed to retrieve data. Status code:", status_code(response)))
}
content_data <- content(response, as = "parsed", type = "application/json")
content_data
# Extracting Author Keywords and Index Keywords
author_keywords <- content_data$`abstracts-retrieval-response`$`authkeywords`$keyword
author_keywords
# Your Scopus API Key
api_key <- "de7b2ffad08bb8df27c949ff986c80be"
# DOI for which you want to retrieve data
doi <- "10.1016/j.compstruct.2020.112233" # Replace with your DOI
# Base URL for Scopus Subject Classifications API
base_url <- "https://api.elsevier.com/content/subject/scopus"
# Example parameters
description <- "biological" # Filter by word in description
http_accept <- "application/json" # Desired response format
# Prepare the request URL with query parameters
request_url <- paste0(base_url, "?apiKey=", api_key, "&httpAccept=", http_accept, "&description=", description)
request_url
# Make the GET request
response <- GET(request_url)
response
# Check if the request was successful
if (status_code(response) == 200) {
# Parse the response content
content_data <- content(response, as = "parsed", type = "application/json")
# Assuming 'content_data' contains the classifications, print or process them here
print(content_data)
} else {
print(paste("Failed to retrieve data. Status code:", status_code(response)))
}
getKeywordsForDOI("10.1016/j.scienta.2004.01.003")
# Define the function to make an API call and extract keywords
getKeywordsForDOI <- function(doi) {
# URL for the Scopus Subject Classifications API
baseURL <- "https://api.elsevier.com/content/subject/scopus"
# Make the API request
response <- GET(url = baseURL,
query = list(httpAccept = "application/json",
apiKey = 'de7b2ffad08bb8df27c949ff986c80be',
doi = doi),
add_headers(Accept = "application/json"))
# Check if the request was successful
if (status_code(response) == 200) {
# Parse the response
content <- content(response, as = "parsed", type = "application/json")
# Extract keywords
keywords <- sapply(content$`subject-classifications`$`subject-classification`, function(x) x$detail, USE.NAMES = FALSE)
return(keywords)
} else {
# Return NULL or an appropriate response if the API call fails
return(NULL)
}
}
getKeywordsForDOI("10.1016/j.scienta.2004.01.003")
View(data_raw)
getKeywordsForDOI("10.1016/j.apsusc.2022.154770")
# Define the function to make an API call and extract keywords
getKeywordsForDOI <- function(doi) {
# URL for the Scopus Subject Classifications API
baseURL <- "https://api.elsevier.com/content/subject/scopus"
# Make the API request
response <- GET(url = baseURL,
query = list(httpAccept = "application/json",
apiKey = 'de7b2ffad08bb8df27c949ff986c80be',
doi = doi),
add_headers(Accept = "application/json"))
# Check if the request was successful
if (status_code(response) == 200) {
# Parse the response
content <- content(response, as = "parsed", type = "application/json")
# Extract keywords
keywords <- sapply(content$`subject-classifications`$`subject-classification`[[6]], function(x) x$detail, USE.NAMES = FALSE)
return(keywords)
} else {
# Return NULL or an appropriate response if the API call fails
return(NULL)
}
}
getKeywordsForDOI("10.1016/j.apsusc.2022.154770")
install.packages('jsonlite')
install.packages('jsonlite')
pacman::p_load(tidyverse, here, gt, rapidraker, httr, jsonlite)
data_ided <- data_raw %>%
# colnames()
select(all_of(cols_needed)) %>%
rowid_to_column()
# Your Scopus API Key
api_key <- "de7b2ffad08bb8df27c949ff986c80be"
# DOI for which you want to retrieve data
doi <- "10.1021/acs.jpca.2c05952" # Replace with your DOI
# DOI for which you want to retrieve data
doi <- "10.1021/acs.jpca.2c05952" # Replace with your DOI
# Function to search by DOI and extract subject categories and author indexes
getScopusDataByDOI <- function(doi) {
# Define the base URL for the Scopus Search API
baseUrl <- "https://api.elsevier.com/content/search/scopus"
# Define the query parameter with the DOI
queryParams <- list(
query = paste0('DOI(', doi, ')'),
field = 'prism:doi,authkeywords,subject-area,dc:identifier',
apiKey = 'de7b2ffad08bb8df27c949ff986c80be',
httpAccept = 'application/json'
)
# Make the GET request to the Scopus API
response <- GET(baseUrl, query = queryParams)
# Check if the request was successful
if (status_code(response) == 200) {
# Parse the JSON response
content <- fromJSON(rawToChar(response$content))
# Extract the needed information from the response
# Note: Adjust the paths based on the actual structure of the JSON response
entries <- content$search-results$entry
if (length(entries) > 0) {
# Assuming the first entry is the relevant one
entry <- entries[[1]]
# Extract Subject Categories and Authors Index
subjectCategories <- sapply(entry$`subject-area`, function(x) x$`$`)
authorIndexes <- entry$authkeywords
return(list(SubjectCategories = subjectCategories, AuthorIndexes = authorIndexes))
} else {
return(NULL)
}
} else {
stop("Failed to fetch data from Scopus API. Status code: ", status_code(response))
}
}
tmp <- getScopusDataByDOI(doi)
tmp <- getScopusDataByDOI(doi)
# Function to search by DOI and extract subject categories and author indexes
# getScopusDataByDOI <- function(doi) {
# Define the base URL for the Scopus Search API
baseUrl <- "https://api.elsevier.com/content/search/scopus"
# Define the query parameter with the DOI
queryParams <- list(
query = paste0('DOI(', doi, ')'),
field = 'prism:doi,authkeywords,subject-area,dc:identifier',
apiKey = 'de7b2ffad08bb8df27c949ff986c80be',
httpAccept = 'application/json'
)
# Make the GET request to the Scopus API
response <- GET(baseUrl, query = queryParams)
response
fromJSON(rawToChar(response$content))
# Parse the JSON response
content <- fromJSON(rawToChar(response$content))
# Extract the needed information from the response
# Note: Adjust the paths based on the actual structure of the JSON response
entries <- content$search-results$entry
# Extract the needed information from the response
# Note: Adjust the paths based on the actual structure of the JSON response
entries <- content$`search-results`$entry
entries
# Assuming the first entry is the relevant one
entry <- entries[[1]]
entry
entries[['prism:doi']]
# Assuming the first entry is the relevant one
entry <- entries[['prism:doi']]
# Extract Subject Categories and Authors Index
subjectCategories <- sapply(entry$`subject-area`, function(x) x$`$`)
data_ided %>%
filter(is.na(author_keywords) & is.na(index_keywords))
### I designed this so that no one need to have an issue dealing with large data files.
### Instead you can simply run this on your machine (the last table is huge it can break your seesion)
pacman::p_load(tidyverse, here)
path <- here('data', 'WoS')
data <- read_tsv(here('data','WoS','savedrecs18001-19000.txt'))
# List all .txt files in the directory
txt_files <- list.files(path = path, pattern = "\\.txt$", full.names = TRUE)
# Read each .txt file into a list of data frames
list_of_data_frames <- map(txt_files, read_tsv)
list_of_data_frames[[1]]
list_of_data_frames[[1]] %>% colnames
list_of_data_frames[[1]] %>% glimpse()
# Define the selected variables and new names in independent vectors
selected_vars <- c("TI", "DT", "PT", "PY", "DI", "AU", "C1", "C3", "DE", "ID", "WC", "SC", "AB", "TC", "SO")
new_names <- c("DocumentTitle", "DocumentType", "PublicationType", "PublicationYear", "DOI",
"Authors", "AuthorAffiliation", "AuthorAffiliation2", "AuthorKeywords", "KeywordsPlus",
"WoSCategories", "SubjectCategory", "Abstract", "TimesCited", "SourcePublication")
# Define a function to filter and rename dataset columns
filter_and_rename <- function(dataset) {
dataset %>%
select(all_of(selected_vars)) %>%
rename_with(~new_names, all_of(selected_vars)) %>%
mutate(
across(
c(TimesCited, PublicationYear),
as.numeric
)
) %>%
janitor::clean_names()
}
wos_data <- list_of_data_frames %>%
map(filter_and_rename) %>%
bind_rows()
# ################################################################################
# ################################################################################
# ################################################################################
# # CLEAN DATA
#
# clean_wos <- read_csv(here('data', 'WoS', 'clean_wos.csv'))
clean_wos <- wos_data %>%
distinct(doi, .keep_all = T) %>%
filter(!is.na(authors) & !is.na(publication_year) & !is.na(author_keywords) &
!is.na(keywords_plus) & !is.na(author_affiliation) & !is.na(wo_s_categories) &
!(publication_year %in% c(25, 30, 64)))
# clean_wos %>% write_csv(here('data', 'WoS', 'clean_wos.csv'))
# clean_wos <- read_csv(here('data', 'WoS', 'clean_wos.csv'))
# adding id to the each paper
wos_ided <- clean_wos %>%
rowid_to_column()
#########################################################
source("~/.active-rstudio-document", echo=TRUE)
wos_ided %>%
select(rowid, author_affiliation, author_affiliation2) %>%
separate_rows(subject_category, sep = "; [\\s*")
wos_ided %>%
select(rowid, author_affiliation, author_affiliation2) %>%
separate_rows(author_affiliation, sep = "; [\\s*")
wos_ided %>%
select(rowid, author_affiliation, author_affiliation2) %>%
mutate(author_affiliation = gsub("; \\[", "|| [", author_affiliation))
wos_ided %>%
select(rowid, author_affiliation, author_affiliation2) %>%
mutate(author_affiliation = gsub("; \\[", "|| [", author_affiliation)) %>%
separate_rows(author_affiliation, sep = "\\|\\| ")
wos_ided %>%
select(rowid, author_affiliation, author_affiliation2) %>%
mutate(author_affiliation = gsub("; \\[", "|| [", author_affiliation)) %>%
separate_rows(author_affiliation, sep = "\\|\\| ") %>% View()
wos_ided %>%
select(rowid, author_affiliation, author_affiliation2) %>%
mutate(author_affiliation = gsub("\\];", "];\n", author_affiliation)) %>%
separate_rows(author_affiliation, sep = ";\\s*|\n")
df <- wos_ided %>%
select(rowid, author_affiliation, author_affiliation2)
# Step 1: Extract Author Names
df$author_names <- str_extract_all(df$AF1, "\\[.*?\\]") %>%
lapply(function(x) str_remove_all(x, "\\[|\\]")) %>%
lapply(paste, collapse = "; ") %>%
unlist()
# Step 1: Extract Author Names
df$author_names <- str_extract_all(df$author_affiliation, "\\[.*?\\]") %>%
lapply(function(x) str_remove_all(x, "\\[|\\]")) %>%
lapply(paste, collapse = "; ") %>%
unlist()
# Step 2: Extract Countries
df$author_country <- str_extract_all(df$author_affiliation, "[^;]+?$") %>%
lapply(function(x) str_extract(x, "\\b\\w+$")) %>%
lapply(paste, collapse = "; ") %>%
unlist()
# Step 3: Extract Institutions from AF2
# Directly splitting by semicolon since it's straightforward
df$author_aff <- str_split(df$author_affiliation2, "; ") %>%
lapply(paste, collapse = "; ") %>%
unlist()
df
View(df)
wos_ided %>%
select(rowid, author_affiliation, author_affiliation2) %>%
mutate(country = str_extract_all(author_affiliation, "(?<=,\\s|\\d\\s)[A-Za-z\\s]+(?=;|$)"))
tmp <- wos_ided %>%
select(rowid, author_affiliation, author_affiliation2) %>%
mutate(country = str_extract_all(author_affiliation, "(?<=,\\s|\\d\\s)[A-Za-z\\s]+(?=;|$)"))
tmp %>%
mutate(country = paste(unlist(country), collapse = "; "))
tmp %>%
mutate(country = paste(unlist(country), collapse = "; ")) %>%  View()
