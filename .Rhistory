pacman::p_load(tidyverse, here, gt, rapidraker, httr)
data_raw <- read_csv(here('data', 'scopus18384-2.csv')) %>%
janitor::clean_names()
cols_needed <- c(
'title',
'year',
'authors',
'cited_by',
'author_keywords',
'index_keywords',
'authors_with_affiliations',
'affiliations',
'abstract'
)
data_ided <- data_raw %>%
# colnames()
select(all_of(cols_needed)) %>%
rowid_to_column()
# Your Scopus API Key
api_key <- "de7b2ffad08bb8df27c949ff986c80be"
# DOI for which you want to retrieve data
doi <- "10.1016/j.compstruct.2020.112233" # Replace with your DOI
# Base URL for Scopus API Abstract Retrieval
base_url <- "https://api.elsevier.com/content/article/doi/"
# Prepare the request URL
request_url <- paste0(base_url, doi, "?apiKey=", api_key, "&httpAccept=application/json")
request_url
# Make the GET request
response <- GET(request_url)
response
# Check if the request was successful
if (status_code(response) == 200) {
content_data <- content(response, as = "parsed", type = "application/json")
# Extracting Author Keywords and Index Keywords
author_keywords <- content_data$`abstracts-retrieval-response`$`authkeywords`$keyword
index_keywords <- NA # Scopus API doesn't directly provide "index keywords" in the same way as Web of Science
# Print the keywords
if (!is.null(author_keywords)) {
print("Author Keywords:")
print(sapply(author_keywords, function(k) k$`$`))
} else {
print("No Author Keywords found.")
}
# For index keywords, you might need to look into other parts of the response
# or use specific fields that could be interpreted as index keywords.
} else {
print(paste("Failed to retrieve data. Status code:", status_code(response)))
}
content_data <- content(response, as = "parsed", type = "application/json")
content_data
# Extracting Author Keywords and Index Keywords
author_keywords <- content_data$`abstracts-retrieval-response`$`authkeywords`$keyword
author_keywords
# Your Scopus API Key
api_key <- "de7b2ffad08bb8df27c949ff986c80be"
# DOI for which you want to retrieve data
doi <- "10.1016/j.compstruct.2020.112233" # Replace with your DOI
# Base URL for Scopus Subject Classifications API
base_url <- "https://api.elsevier.com/content/subject/scopus"
# Example parameters
description <- "biological" # Filter by word in description
http_accept <- "application/json" # Desired response format
# Prepare the request URL with query parameters
request_url <- paste0(base_url, "?apiKey=", api_key, "&httpAccept=", http_accept, "&description=", description)
request_url
# Make the GET request
response <- GET(request_url)
response
# Check if the request was successful
if (status_code(response) == 200) {
# Parse the response content
content_data <- content(response, as = "parsed", type = "application/json")
# Assuming 'content_data' contains the classifications, print or process them here
print(content_data)
} else {
print(paste("Failed to retrieve data. Status code:", status_code(response)))
}
getKeywordsForDOI("10.1016/j.scienta.2004.01.003")
# Define the function to make an API call and extract keywords
getKeywordsForDOI <- function(doi) {
# URL for the Scopus Subject Classifications API
baseURL <- "https://api.elsevier.com/content/subject/scopus"
# Make the API request
response <- GET(url = baseURL,
query = list(httpAccept = "application/json",
apiKey = 'de7b2ffad08bb8df27c949ff986c80be',
doi = doi),
add_headers(Accept = "application/json"))
# Check if the request was successful
if (status_code(response) == 200) {
# Parse the response
content <- content(response, as = "parsed", type = "application/json")
# Extract keywords
keywords <- sapply(content$`subject-classifications`$`subject-classification`, function(x) x$detail, USE.NAMES = FALSE)
return(keywords)
} else {
# Return NULL or an appropriate response if the API call fails
return(NULL)
}
}
getKeywordsForDOI("10.1016/j.scienta.2004.01.003")
View(data_raw)
getKeywordsForDOI("10.1016/j.apsusc.2022.154770")
# Define the function to make an API call and extract keywords
getKeywordsForDOI <- function(doi) {
# URL for the Scopus Subject Classifications API
baseURL <- "https://api.elsevier.com/content/subject/scopus"
# Make the API request
response <- GET(url = baseURL,
query = list(httpAccept = "application/json",
apiKey = 'de7b2ffad08bb8df27c949ff986c80be',
doi = doi),
add_headers(Accept = "application/json"))
# Check if the request was successful
if (status_code(response) == 200) {
# Parse the response
content <- content(response, as = "parsed", type = "application/json")
# Extract keywords
keywords <- sapply(content$`subject-classifications`$`subject-classification`[[6]], function(x) x$detail, USE.NAMES = FALSE)
return(keywords)
} else {
# Return NULL or an appropriate response if the API call fails
return(NULL)
}
}
getKeywordsForDOI("10.1016/j.apsusc.2022.154770")
install.packages('jsonlite')
install.packages('jsonlite')
pacman::p_load(tidyverse, here, gt, rapidraker, httr, jsonlite)
data_ided <- data_raw %>%
# colnames()
select(all_of(cols_needed)) %>%
rowid_to_column()
# Your Scopus API Key
api_key <- "de7b2ffad08bb8df27c949ff986c80be"
# DOI for which you want to retrieve data
doi <- "10.1021/acs.jpca.2c05952" # Replace with your DOI
# DOI for which you want to retrieve data
doi <- "10.1021/acs.jpca.2c05952" # Replace with your DOI
# Function to search by DOI and extract subject categories and author indexes
getScopusDataByDOI <- function(doi) {
# Define the base URL for the Scopus Search API
baseUrl <- "https://api.elsevier.com/content/search/scopus"
# Define the query parameter with the DOI
queryParams <- list(
query = paste0('DOI(', doi, ')'),
field = 'prism:doi,authkeywords,subject-area,dc:identifier',
apiKey = 'de7b2ffad08bb8df27c949ff986c80be',
httpAccept = 'application/json'
)
# Make the GET request to the Scopus API
response <- GET(baseUrl, query = queryParams)
# Check if the request was successful
if (status_code(response) == 200) {
# Parse the JSON response
content <- fromJSON(rawToChar(response$content))
# Extract the needed information from the response
# Note: Adjust the paths based on the actual structure of the JSON response
entries <- content$search-results$entry
if (length(entries) > 0) {
# Assuming the first entry is the relevant one
entry <- entries[[1]]
# Extract Subject Categories and Authors Index
subjectCategories <- sapply(entry$`subject-area`, function(x) x$`$`)
authorIndexes <- entry$authkeywords
return(list(SubjectCategories = subjectCategories, AuthorIndexes = authorIndexes))
} else {
return(NULL)
}
} else {
stop("Failed to fetch data from Scopus API. Status code: ", status_code(response))
}
}
tmp <- getScopusDataByDOI(doi)
tmp <- getScopusDataByDOI(doi)
# Function to search by DOI and extract subject categories and author indexes
# getScopusDataByDOI <- function(doi) {
# Define the base URL for the Scopus Search API
baseUrl <- "https://api.elsevier.com/content/search/scopus"
# Define the query parameter with the DOI
queryParams <- list(
query = paste0('DOI(', doi, ')'),
field = 'prism:doi,authkeywords,subject-area,dc:identifier',
apiKey = 'de7b2ffad08bb8df27c949ff986c80be',
httpAccept = 'application/json'
)
# Make the GET request to the Scopus API
response <- GET(baseUrl, query = queryParams)
response
fromJSON(rawToChar(response$content))
# Parse the JSON response
content <- fromJSON(rawToChar(response$content))
# Extract the needed information from the response
# Note: Adjust the paths based on the actual structure of the JSON response
entries <- content$search-results$entry
# Extract the needed information from the response
# Note: Adjust the paths based on the actual structure of the JSON response
entries <- content$`search-results`$entry
entries
# Assuming the first entry is the relevant one
entry <- entries[[1]]
entry
entries[['prism:doi']]
# Assuming the first entry is the relevant one
entry <- entries[['prism:doi']]
# Extract Subject Categories and Authors Index
subjectCategories <- sapply(entry$`subject-area`, function(x) x$`$`)
data_ided %>%
filter(is.na(author_keywords) & is.na(index_keywords))
